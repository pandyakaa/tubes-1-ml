{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tugas Besar 1 - Bagian D \n",
    "\n",
    "## Kelompok XX K- 3 \n",
    "1. Pandyaka Aptanagi - 13517003\n",
    "2. I Putu Gede Wirasuta - 13517015\n",
    "3. M. Rifky I. Bariansya - 13517081\n",
    "4. Gardahadi - 13517144"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## I. Import Library Standar dan Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Standard Libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import DTL Classifier\n",
    "from clf.mlp import MyMlp\n",
    "from clf.c45 import C45\n",
    "from clf.utils import read_csv, scale_data\n",
    "from clf.c45_numeric_handler import process_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## II. Mendefinisikan Fungsi Bantuan "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function Import Data\n",
    "\n",
    "def import_data(path, sep=',',dropna=True, drop_duplicates=True):\n",
    "    data = pd.read_csv(path, sep)\n",
    "    print(\"Import Berhasil\")\n",
    "    print(\"Banyaknya baris dan kolom\", data.shape)\n",
    "    print(\"Banyaknya data duplicate\", data.duplicated().sum())\n",
    "    data = data.drop_duplicates() if drop_duplicates else data\n",
    "    data = data.dropna() if dropna else data\n",
    "#     print(data.head())\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to Extract Input and Output\n",
    "\n",
    "def extract_feature_target(data, output):\n",
    "    y = data[output]\n",
    "    x = data.drop(output, axis=1)\n",
    "    #print(x.columns) #optional\n",
    "    #print(y.head()) #optional\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to split train test data (default test_size = 0.4)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def train_test_data(x,y, _test_size = 0.4): \n",
    "    x_train, x_test, y_train, y_test = train_test_split(x,y, test_size =_test_size,\n",
    "                                                        random_state =100)\n",
    "    print(\"Training Data :\")\n",
    "    print(x_train.head(), y_train.head()) #\n",
    "    print(\" Testing Data :\" )\n",
    "    print(x_test.head(), y_test.head()) #\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to evaluate score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def scoring(model, x_test, y_test) :\n",
    "    y_predict = model.predict(x_test)\n",
    "    print(\"Model Score based on test data\")\n",
    "    print(accuracy_score(y_test, y_predict))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagian 1 : Eksperimen Skema Hold-out Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1.1 Pembelajaran DTL"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1.2 Pembelajaran ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagian 2 : Eksperimen Skema 10-fold Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2.1 Pembelajaran DTL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import things\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "df = import_data('../Bagian C/datasets/iris.csv',',')\n",
    "x, y = extract_feature_target(df, ' species')\n",
    "\n",
    "input_layer = len(list(x))\n",
    "output_layer = len(set(y))\n",
    "hidden_layer = [4, 3]\n",
    "\n",
    "# Create MyC45\n",
    "myc45 = C45()\n",
    "myc45.label = list(x.columns)\n",
    "x_temp = process_numeric(np.vectorize(str)(x.values), y.values)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('clf', myc45)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtl_cross_validate = cross_validate(pipeline, x_temp, y.values, cv=10, scoring='accuracy')\n",
    "\n",
    "for key, values in dtl_cross_validate.items():\n",
    "        print(key,' mean ', values.mean())\n",
    "        print(key,' std ', values.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2.2 Pembelajaran ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create myMLP\n",
    "myMLP = MyMlp(input_layer, hidden_layer, output_layer, epochs=1000, learning_rate=0.5)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('clf', myMLP)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_cross_validate = cross_validate(pipeline, x.values, y.values, cv=10, scoring='accuracy')\n",
    "\n",
    "for key, values in ann_cross_validate.items():\n",
    "        print(key,' mean ', values.mean())\n",
    "        print(key,' std ', values.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagian 3 : Penyimpanan dan Pemanggilan Model"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3.1 Menyimpan Model dalam file .txt"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3.2 Memanggil Model "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagian 4 : Analisis Eksperimen Bagian 1 & 2"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = import_data('../Bagian C/datasets/iris.csv',',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = extract_feature_target(df, ' species')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "# from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "num_attribs = list(x)\n",
    "\n",
    "\n",
    "preprocessor = Pipeline([\n",
    "    # ('selector', DataFrameSelector(num_attribs)),\n",
    "#     ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "    # ('attribs_adder', CombinedAttributesAdder()),\n",
    "    ('std_scaler', StandardScaler()),\n",
    "  ])\n",
    "\n",
    "# cat_pipeline = Pipeline([\n",
    "#     ('selector', DataFrameSelector(cat_attribs)),\n",
    "#     ('label_binarizer', MyLabelBinarizer()),\n",
    "#   ])\n",
    "\n",
    "# preprocessor = FeatureUnion(transformer_list=[\n",
    "#     (\"num_pipeline\", num_pipeline),\n",
    "#     (\"cat_pipeline\", cat_pipeline),\n",
    "#   ])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clf.mlp import MyMlp\n",
    "from clf.c45 import C45\n",
    "from clf.utils import read_csv, scale_data\n",
    "from clf.c45_numeric_handler import process_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = len(list(x))\n",
    "print(input_layer)\n",
    "output_layer = len(set(y))\n",
    "print(output_layer)\n",
    "hidden_layer = [4, 3]\n",
    "print(hidden_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myclf = MyMlp(input_layer, hidden_layer, output_layer, epochs=2500, learning_rate=0.02)\n",
    "myc45 = C45()\n",
    "myc45.label = list(x.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "pipeline = Pipeline([\n",
    "    # ('processor', preprocessor), #Step1 - preprocess data\n",
    "    ('clf', LogisticRegression()) #step2 - classifier (default : LogisticRegression )\n",
    "])\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "clfs = []\n",
    "# clfs.append(myclf)\n",
    "clfs.append(myc45)\n",
    "# clfs.append(SVC())\n",
    "# clfs.append(KNeighborsClassifier(n_neighbors=3))\n",
    "# clfs.append(DecisionTreeClassifier())\n",
    "# clfs.append(RandomForestClassifier())\n",
    "# clfs.append(GradientBoostingClassifier())\n",
    "\n",
    "for classifier in clfs:\n",
    "    pipeline.set_params(clf = classifier)\n",
    "    x_temp = process_numeric(np.vectorize(str)(x.values), y.values)\n",
    "    print(x_temp)\n",
    "    scores = cross_validate(pipeline, x_temp, y.values, cv=5, scoring='accuracy')\n",
    "    print('---------------------------------')\n",
    "    print(str(classifier))\n",
    "    print('-----------------------------------')\n",
    "    for key, values in scores.items():\n",
    "            print(key,' mean ', values.mean())\n",
    "            print(key,' std ', values.std())\n",
    "    print(scores)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.7.2 64-bit",
   "language": "python",
   "name": "python37264bit0c9ce3831b1c4d0782eb0dd4d688b280"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}